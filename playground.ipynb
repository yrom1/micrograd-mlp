{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import Neuron, Layer, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MLP of [Layer of [ReLUNeuron(3)], Layer of [LinearNeuron(1)]],\n",
       " MLP of [Layer of [ReLUNeuron(3), ReLUNeuron(3), ReLUNeuron(3)], Layer of [LinearNeuron(3)]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = MLP(3, [1, 1])\n",
    "m = MLP(3, [3, 1])\n",
    "n, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _: # MLP\n",
    "    from typing import List\n",
    "    def __init__(self, nin: int, nouts: List[int]):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [\n",
    "            Layer(sz[i], sz[i + 1], nonlin=i != len(nouts) - 1)\n",
    "            for i in range(len(nouts))\n",
    "        ]\n",
    "# MLP CONSTRUCTOR\n",
    "# this is straightforward right, we make layers of\n",
    "# cin, cout combos but we only make #cout of them\n",
    "# each layers takes nin starting with nin, but then\n",
    "# all other layers take the nouts[i] of the previous\n",
    "\n",
    "# LINEAR OUTPUT\n",
    "# the last layers is linear so we can predict any number not just [0,int)\n",
    "\n",
    "# RELU LAYERS\n",
    "# ReLU (Rectified Linear Unit) introduces non-linearity into neural networks\n",
    "# due to its non-linear nature. The ReLU function is defined as:\n",
    "#\n",
    "# f(x) = max(0, x)\n",
    "#\n",
    "# without relus (or any non linear activation) nn's would be linear funtions\n",
    "\n",
    "#  By definition, the ReLU is ð‘šð‘Žð‘¥(0,ð‘¥). Therefore, if we split the domain from\n",
    "# (âˆ’âˆž,0] or [0,âˆž), then the function is linear. However, it's easy to see\n",
    "# that ð‘“(âˆ’1)+ð‘“(1)â‰ ð‘“(0). Hence, by definition, ReLU is not linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internals(n: MLP) -> None:\n",
    "  for layer in n.layers:\n",
    "    print(layer, '---')\n",
    "    for neuron in layer.neurons:\n",
    "      print(neuron, '*')\n",
    "      for value in neuron.parameters(): # .w and [.b]\n",
    "        print(value, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer of [ReLUNeuron(3)] ---\n",
      "ReLUNeuron(3) *\n",
      "Value(data=0.23550571390294128, grad=0) .\n",
      "Value(data=0.06653114721000164, grad=0) .\n",
      "Value(data=-0.26830328150124894, grad=0) .\n",
      "Value(data=0, grad=0) .\n",
      "Layer of [LinearNeuron(1)] ---\n",
      "LinearNeuron(1) *\n",
      "Value(data=0.1715747078045431, grad=0) .\n",
      "Value(data=0, grad=0) .\n"
     ]
    }
   ],
   "source": [
    "internals(n)\n",
    "# MLP Diagram:\n",
    "#\n",
    "#   Input (3 features)\n",
    "#       â†“\n",
    "# Layer 1 (ReLUNeuron) # w: [0.2, 0.1, 0.3], b: 0.0\n",
    "#       â†“\n",
    "# Layer 2 (LinearNeuron) # w: [0.2], b: 0.0\n",
    "#       â†“\n",
    "#  Output (1 output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer of [ReLUNeuron(3), ReLUNeuron(3), ReLUNeuron(3)] ---\n",
      "ReLUNeuron(3) *\n",
      "Value(data=-0.6686254326224383, grad=0) .\n",
      "Value(data=0.6487474938152629, grad=0) .\n",
      "Value(data=-0.23259038277158273, grad=0) .\n",
      "Value(data=0, grad=0) .\n",
      "ReLUNeuron(3) *\n",
      "Value(data=0.5792256498313748, grad=0) .\n",
      "Value(data=0.8434530197925192, grad=0) .\n",
      "Value(data=-0.3847332240409951, grad=0) .\n",
      "Value(data=0, grad=0) .\n",
      "ReLUNeuron(3) *\n",
      "Value(data=0.9844941451716409, grad=0) .\n",
      "Value(data=-0.5901079958448365, grad=0) .\n",
      "Value(data=0.31255526637777775, grad=0) .\n",
      "Value(data=0, grad=0) .\n",
      "Layer of [LinearNeuron(3)] ---\n",
      "LinearNeuron(3) *\n",
      "Value(data=0.8246106857787521, grad=0) .\n",
      "Value(data=-0.7814232047574572, grad=0) .\n",
      "Value(data=0.6408752595662697, grad=0) .\n",
      "Value(data=0, grad=0) .\n"
     ]
    }
   ],
   "source": [
    "internals(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
