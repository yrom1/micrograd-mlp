{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import * # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import Neuron, Layer, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MLP of [Layer of [ReLUNeuron(3)], Layer of [LinearNeuron(1)]],\n",
       " MLP of [Layer of [ReLUNeuron(3), ReLUNeuron(3)], Layer of [LinearNeuron(2)]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = MLP(3, [1, 1])\n",
    "m = MLP(3, [2, 1])\n",
    "n, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _: # MLP\n",
    "    from typing import List\n",
    "    def __init__(self, nin: int, nouts: List[int]):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [\n",
    "            Layer(sz[i], sz[i + 1], nonlin=i != len(nouts) - 1)\n",
    "            for i in range(len(nouts))\n",
    "        ]\n",
    "# MLP CONSTRUCTOR\n",
    "# this is straightforward right, we make layers of\n",
    "# cin, cout combos but we only make #cout of them\n",
    "# each layers takes nin starting with nin, but then\n",
    "# all other layers take the nouts[i] of the previous\n",
    "\n",
    "# LINEAR OUTPUT\n",
    "# the last layers is linear so we can predict any number not just [0,int)\n",
    "\n",
    "# RELU LAYERS\n",
    "# ReLU (Rectified Linear Unit) introduces non-linearity into neural networks\n",
    "# due to its non-linear nature. The ReLU function is defined as:\n",
    "#\n",
    "# f(x) = max(0, x)\n",
    "#\n",
    "# without relus (or any non linear activation) nn's would be linear funtions\n",
    "\n",
    "#  By definition, the ReLU is 𝑚𝑎𝑥(0,𝑥). Therefore, if we split the domain from\n",
    "# (−∞,0] or [0,∞), then the function is linear. However, it's easy to see\n",
    "# that 𝑓(−1)+𝑓(1)≠𝑓(0). Hence, by definition, ReLU is not linear. \n",
    "# https://datascience.stackexchange.com/a/26481\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internals(n: MLP) -> None:\n",
    "  for layer in n.layers:\n",
    "    print(layer, '---')\n",
    "    for neuron in layer.neurons:\n",
    "      print(neuron, '*')\n",
    "      for value in neuron.parameters(): # .w and [.b]\n",
    "        print(value, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer of [ReLUNeuron(3)] ---\n",
      "ReLUNeuron(3) *\n",
      "Value(data=0.23550571390294128, grad=0) .\n",
      "Value(data=0.06653114721000164, grad=0) .\n",
      "Value(data=-0.26830328150124894, grad=0) .\n",
      "Value(data=0, grad=0) .\n",
      "Layer of [LinearNeuron(1)] ---\n",
      "LinearNeuron(1) *\n",
      "Value(data=0.1715747078045431, grad=0) .\n",
      "Value(data=0, grad=0) .\n"
     ]
    }
   ],
   "source": [
    "internals(n)\n",
    "# MLP Diagram:\n",
    "#\n",
    "#   Input (3 features)\n",
    "#       ↓\n",
    "# Layer 1 (ReLUNeuron) # w: [0.2, 0.1, 0.3], b: 0.0\n",
    "#       ↓\n",
    "# Layer 2 (LinearNeuron) # w: [0.2], b: 0.0\n",
    "#       ↓\n",
    "#  Output (1 output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer of [ReLUNeuron(3), ReLUNeuron(3)] ---\n",
      "ReLUNeuron(3) *\n",
      "Value(data=-0.6686254326224383, grad=0) .\n",
      "Value(data=0.6487474938152629, grad=0) .\n",
      "Value(data=-0.23259038277158273, grad=0) .\n",
      "Value(data=0, grad=0) .\n",
      "ReLUNeuron(3) *\n",
      "Value(data=0.5792256498313748, grad=0) .\n",
      "Value(data=0.8434530197925192, grad=0) .\n",
      "Value(data=-0.3847332240409951, grad=0) .\n",
      "Value(data=0, grad=0) .\n",
      "Layer of [LinearNeuron(2)] ---\n",
      "LinearNeuron(2) *\n",
      "Value(data=0.9844941451716409, grad=0) .\n",
      "Value(data=-0.5901079958448365, grad=0) .\n",
      "Value(data=0, grad=0) .\n"
     ]
    }
   ],
   "source": [
    "internals(m)\n",
    "# here's why this makes sense\n",
    "# you take three inputs, every neuron needs 3 weights plus a bias\n",
    "# but the number outputs of the first layer is two\n",
    "# so you need two neurons in the first layer\n",
    "# the linear layer gives two inputs and thus has two weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 7.256586208060731 ['1.71', '0.14', '1.19', '0.19']\n",
      "2 4.663089955756583 ['-0.55', '-0.48', '-0.58', '-0.35']\n",
      "3 3.604799237460308 ['-0.06', '-0.28', '-0.20', '-0.15']\n",
      "4 3.3813552469202715 ['0.02', '-0.26', '-0.14', '-0.07']\n",
      "5 3.1877095502118094 ['0.05', '-0.29', '-0.14', '-0.02']\n",
      "6 3.058174697261023 ['-0.01', '-0.33', '-0.19', '0.03']\n",
      "7 2.589567291131887 ['0.17', '-0.47', '-0.14', '0.06']\n",
      "8 2.1974764529417348 ['0.26', '-0.58', '-0.16', '0.12']\n",
      "9 2.262004527664124 ['-0.02', '-0.75', '-0.34', '0.15']\n",
      "10 1.545379542101279 ['0.45', '-0.83', '-0.18', '0.27']\n",
      "11 1.1205726782305505 ['0.61', '-0.90', '-0.29', '0.33']\n",
      "12 0.7617626100877495 ['0.63', '-0.86', '-0.53', '0.38']\n",
      "13 0.43591145058177 ['0.89', '-0.94', '-0.58', '0.50']\n",
      "14 0.24574837373975458 ['0.86', '-0.95', '-0.88', '0.54']\n",
      "15 0.18104420395966012 ['1.19', '-0.95', '-0.83', '0.66']\n",
      "16 0.14074749247945073 ['1.07', '-1.04', '-1.02', '0.63']\n",
      "17 0.09826102960666201 ['1.05', '-0.93', '-0.99', '0.70']\n",
      "18 0.069977062653079 ['1.01', '-1.01', '-1.01', '0.74']\n",
      "19 0.055415828865719295 ['1.11', '-0.95', '-0.96', '0.80']\n",
      "20 0.08360025233466223 ['0.83', '-1.03', '-1.08', '0.79']\n",
      "21 0.38484035813305073 ['1.55', '-0.89', '-0.75', '0.93']\n",
      "22 2.3879167812099733 ['-0.40', '-1.24', '-1.49', '0.63']\n",
      "23 3.902768096797196 ['1.98', '-0.43', '0.58', '1.33']\n",
      "24 3.842487495849709 ['-0.20', '-2.17', '-0.45', '0.14']\n",
      "25 2.342941305854181 ['0.84', '0.37', '-0.44', '0.64']\n",
      "26 0.9477157230964726 ['0.72', '-0.21', '-0.94', '0.51']\n",
      "27 0.3285279858007429 ['0.75', '-0.76', '-1.00', '0.54']\n",
      "28 0.1506376953990948 ['0.89', '-0.97', '-0.95', '0.63']\n",
      "29 0.08612036816018084 ['1.01', '-0.95', '-0.95', '0.71']\n",
      "30 0.06293121860233855 ['1.06', '-1.00', '-0.98', '0.76']\n",
      "31 0.0513003031903578 ['1.08', '-0.98', '-0.98', '0.79']\n",
      "32 0.04265433585582224 ['1.08', '-1.00', '-0.99', '0.81']\n",
      "33 0.035440210098933805 ['1.08', '-0.99', '-0.99', '0.83']\n",
      "34 0.02933801818170935 ['1.07', '-1.00', '-0.99', '0.84']\n",
      "35 0.02418537207822451 ['1.06', '-1.00', '-0.99', '0.86']\n",
      "36 0.019857099088356717 ['1.06', '-1.00', '-0.99', '0.87']\n",
      "37 0.016237121474581073 ['1.05', '-1.00', '-0.99', '0.88']\n",
      "38 0.01471792628197933 ['1.04', '-1.00', '-0.99', '0.89']\n",
      "39 0.013451926810391254 ['1.06', '-0.99', '-0.99', '0.90']\n",
      "40 0.012485816872701061 ['1.05', '-1.01', '-1.00', '0.90']\n",
      "41 0.011624863183201477 ['1.06', '-0.99', '-1.00', '0.91']\n",
      "42 0.011380698843872737 ['1.04', '-1.01', '-1.00', '0.90']\n",
      "43 0.010441674745899642 ['1.05', '-0.98', '-0.99', '0.91']\n",
      "44 0.009694170932363595 ['1.04', '-1.01', '-1.00', '0.91']\n",
      "45 0.009014934628775083 ['1.05', '-0.99', '-0.99', '0.92']\n",
      "46 0.008382408965667068 ['1.04', '-1.01', '-1.00', '0.92']\n",
      "47 0.007801504584312247 ['1.05', '-0.99', '-1.00', '0.93']\n",
      "48 0.007259964184461978 ['1.04', '-1.01', '-1.00', '0.92']\n",
      "49 0.006761288869475843 ['1.04', '-0.99', '-1.00', '0.93']\n",
      "50 0.006296123707881714 ['1.04', '-1.01', '-1.00', '0.93']\n"
     ]
    }
   ],
   "source": [
    "nn = MLP(3, [4, 4, 1]) # i think this ignores our random seed sadly\n",
    "for _ in range(50):\n",
    "  ypred = [nn(x) for x in xs]\n",
    "  loss = sum((a - b)**2 for a, b in zip(ypred, ys))\n",
    "  for p in nn.parameters():\n",
    "      p.grad = 0.0\n",
    "  loss.backward()\n",
    "  for p in nn.parameters():\n",
    "      p.data += -0.05 * p.grad\n",
    "  print(_+1, loss.data, [f\"{x.data:1.2f}\" for x in ypred])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
